{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "KhciWx1qP3sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TZntQAKbI_Ur"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6NtxHYfKVv_",
        "outputId": "99339ba5-3282-4fb9-983f-8fad144ee4ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "  \"\"\"\n",
        "  Preprocesses texts to tokenized and lemmatized format\n",
        "  Save preprocessed DataFrame through get_preprocessed_df()\n",
        "  \"\"\"\n",
        "  def __init__(self, df):\n",
        "    self.nlp = spacy.load(\"ru_core_news_sm\")\n",
        "    self.stop_words = stopwords.words('russian')\n",
        "    self.preprocessed = df\n",
        "\n",
        "    self.preprocessed['tokens'], self.preprocessed['tokens_without_stops'] = zip(*self.preprocessed['transcript'].apply(self.tokenize))\n",
        "    self.preprocessed['lemmas'], self.preprocessed['lemmas_without_stops'] = zip(*self.preprocessed['transcript'].apply(self.lemmatize))\n",
        "\n",
        "    self.preprocessed.columns = ['ID',\n",
        "                                'fileID',\n",
        "                                'transcript',\n",
        "                                'discourse.type',\n",
        "                                'stimulus',\n",
        "                                'time.point',\n",
        "                                'tokens',\n",
        "                                'tokens_without_stops',\n",
        "                                'lemmas',\n",
        "                                'lemmas_without_stops']\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"\n",
        "    Getting all tokens, except punctuation marks\n",
        "    Return: list of tokens with stopwords, list of tokens without stopwords\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_w_stops = ', '.join([i.lower() for i in tokens if (i not in punctuation)])\n",
        "    tokens_wo_stops = ', '.join([i.lower() for i in tokens if (i not in punctuation) and (i not in self.stop_words)])\n",
        "    return tokens_w_stops, tokens_wo_stops\n",
        "\n",
        "  def lemmatize(self, text):\n",
        "    \"\"\"\n",
        "    Getting lemmas from text with and without stopwords\n",
        "    Return: list of lemmas with stopwords, list of lemmas without stopwords\n",
        "    \"\"\"\n",
        "    doc = self.nlp(text)\n",
        "    lemmas = ', '.join([token.lemma_.lower() for token in doc if (token.text not in punctuation)])\n",
        "    lemmas_without_stops = ', '.join([token.lemma_.lower() for token in doc if (token.text not in punctuation) and (token.text not in self.stop_words)])\n",
        "    return lemmas, lemmas_without_stops\n",
        "\n",
        "  def get_preprocessed_df(self):\n",
        "    \"\"\"\n",
        "    Getter for preprocessed dataframe\n",
        "    Return: pd.DataFrame\n",
        "    \"\"\"\n",
        "    return self.preprocessed\n",
        "\n",
        "  def save_preprocessed_df(self, path):\n",
        "    \"\"\"\n",
        "    Save preprocessed DataFrame to given path\n",
        "    Return: None\n",
        "    \"\"\"\n",
        "    self.preprocessed.to_excel(path)"
      ],
      "metadata": {
        "id": "YpnyuCdfHd_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
