{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "import numpy as np\n",
        "from itertools import permutations\n",
        "import json"
      ],
      "metadata": {
        "id": "hQPHWaSnBMLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataExtractionBase:\n",
        "\n",
        "    def __init__(self, link: str) -> None:\n",
        "        self.category_types = None\n",
        "\n",
        "    def get_ids(self, sheet_name: str):\n",
        "        pass\n",
        "\n",
        "    def get_column_name(self, category: str):\n",
        "        pass\n",
        "\n",
        "    def get_series(self, sheet_name: str, category: str):\n",
        "        pass\n",
        "\n",
        "\n",
        "class DataExtractionPDTexts(DataExtractionBase):\n",
        "\n",
        "    def __init__(self, link: str) -> None:\n",
        "        super().__init__(link)\n",
        "        self.dataset_norm = pd.read_excel(link, sheet_name='healthy')\n",
        "        self.dataset_pd = pd.read_excel(link, sheet_name='general_massive')\n",
        "        self.category_types = ['tokens', 'tokens_without_stops', 'lemmas', 'lemmas_without_stops']\n",
        "\n",
        "    def get_ids(self, sheet_name: str = 'healthy') -> int:\n",
        "        \"\"\"\n",
        "        Getting ID column\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            return self.dataset_norm['speakerID']\n",
        "        return self.dataset_pd['ID']\n",
        "\n",
        "    def get_series(self,\n",
        "                   sheet_name: str,\n",
        "                   category: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Getting one of 8 columns:\n",
        "          from one of the 2 pages of the dataset\n",
        "          from one of the 4 categories\n",
        "\n",
        "        sheet_name: healthy | PD\n",
        "        category: tokens | tokens_without_stops | lemmas | lemmas_without_stops\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            return self.dataset_norm[category]\n",
        "\n",
        "        return self.dataset_pd[category]"
      ],
      "metadata": {
        "id": "cUiVVTGpBpw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClustersDataBase:\n",
        "\n",
        "    def __init__(self,\n",
        "                 extractor: DataExtractionBase,\n",
        "                 model: gensim.models.fasttext.FastTextKeyedVectors) -> None:\n",
        "        self.extractor = extractor\n",
        "        self.model = model\n",
        "        self.healthy_data = None\n",
        "        self.impediment_data = None\n",
        "        self.impediment_type = ''\n",
        "\n",
        "    def get_df(self, sheet):\n",
        "        pass\n",
        "\n",
        "    def add_column(self,\n",
        "                   sheet_name: str,\n",
        "                   category: str,\n",
        "                   clusters: pd.Series) -> None:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def avg_cluster_size(row: pd.Series) -> float:\n",
        "        \"\"\"\n",
        "        Get average cluster size in a row\n",
        "        \"\"\"\n",
        "        clusters_sizes = []\n",
        "        for cell in row:\n",
        "            clusters_sizes.extend(len(cluster) for cluster in cell)\n",
        "        return sum(clusters_sizes) / len(clusters_sizes)\n",
        "\n",
        "    def avg_cluster_distance(self, cluster_sequence):\n",
        "        \"\"\"\n",
        "        Count average cluster distance\n",
        "        \"\"\"\n",
        "        if not cluster_sequence:\n",
        "            return np.NaN\n",
        "\n",
        "        centroids_dict = {}\n",
        "        distances = []\n",
        "\n",
        "        for cluster in cluster_sequence:\n",
        "            centroid = sum(self.model[word] for word in cluster) / len(cluster)\n",
        "            centroids_dict[tuple(cluster)] = centroid\n",
        "\n",
        "        for idx in range(0, len(cluster_sequence)-1):\n",
        "            cluster_1 = cluster_sequence[idx]\n",
        "            cluster_2 = cluster_sequence[idx+1]\n",
        "            Dij = np.dot(\n",
        "                gensim.matutils.unitvec(centroids_dict[tuple(cluster_1)]),\n",
        "                gensim.matutils.unitvec(centroids_dict[tuple(cluster_2)])\n",
        "            )\n",
        "            distances.append(Dij)\n",
        "\n",
        "        if not distances:\n",
        "            return np.NaN\n",
        "\n",
        "        return sum(distances)/len(distances)\n",
        "\n",
        "    def silhouette_score(self, cluster_sequence):\n",
        "        silhouette_coefs = []\n",
        "\n",
        "        for idx, cluster in enumerate(cluster_sequence):\n",
        "            for word_1 in cluster:\n",
        "\n",
        "                a = sum(self.model.similarity(word_1, word_2)\n",
        "                        for word_2 in cluster if word_1 != word_2) / len(cluster)\n",
        "\n",
        "                if idx != len(cluster_sequence) - 1:\n",
        "                    b = sum(self.model.similarity(word_1, word_2)\n",
        "                            for word_2 in cluster_sequence[idx + 1]) / len(cluster_sequence[idx + 1])\n",
        "                else:\n",
        "                    b = sum(self.model.similarity(word_1, word_2)\n",
        "                            for word_2 in cluster_sequence[idx - 1]) / len(cluster_sequence[idx - 1])\n",
        "\n",
        "                s = (b - a) / max(a, b)\n",
        "                silhouette_coefs.append(s)\n",
        "\n",
        "        if silhouette_coefs:\n",
        "            return sum(silhouette_coefs) / len(silhouette_coefs)\n",
        "        return np.NaN\n",
        "\n",
        "    @staticmethod\n",
        "    def cluster_t_score(f_n, f_c, f_nc, N):\n",
        "        if f_nc == 0:\n",
        "            return 0\n",
        "        numerator = f_nc - f_n * f_c / N\n",
        "        denominator = np.sqrt(f_nc)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def avg_cluster_t_score(self, cell, column_clusters):\n",
        "        all_words = ' '.join([word for cell in column_clusters for cluster in cell for word in cluster])\n",
        "        N = len(all_words)\n",
        "\n",
        "        cell_t_scores = []\n",
        "        for cluster in cell:\n",
        "            all_wordpairs = list(permutations(cluster, 2))\n",
        "\n",
        "            pairwise_t_scores = []\n",
        "            for wordpair in all_wordpairs:\n",
        "                f_n = all_words.count(wordpair[0])\n",
        "                f_c = all_words.count(wordpair[1])\n",
        "                f_nc = all_words.count(' '.join((wordpair[0], wordpair[1])))\n",
        "                f_nc += all_words.count(' '.join((wordpair[1], wordpair[0])))\n",
        "\n",
        "                t_score = self.cluster_t_score(f_n, f_c, f_nc, N)\n",
        "                pairwise_t_scores.append(t_score)\n",
        "\n",
        "            cell_t_scores.extend(pairwise_t_scores)\n",
        "\n",
        "        return sum(cell_t_scores)\n",
        "\n",
        "    def save_excel(self, path) -> None:\n",
        "        \"\"\"\n",
        "        Saving data with clusters to an Excel file\n",
        "        \"\"\"\n",
        "        with pd.ExcelWriter(path) as writer:\n",
        "            self.healthy_data.to_excel(writer, sheet_name='healthy', index=False)\n",
        "            self.impediment_data.to_excel(writer, sheet_name=self.impediment_type, index=False)\n",
        "\n",
        "\n",
        "class ClustersDataPDTexts(ClustersDataBase):\n",
        "\n",
        "    def __init__(self,\n",
        "                 extractor: DataExtractionPDTexts,\n",
        "                 model: gensim.models.fasttext.FastTextKeyedVectors) -> None:\n",
        "        super().__init__(extractor, model)\n",
        "        self.id_healthy = extractor.get_ids('healthy')\n",
        "        self.id_impediment = extractor.get_ids('general_massive')\n",
        "        self.healthy_data = pd.DataFrame(self.id_healthy)\n",
        "        self.impediment_data = pd.DataFrame(self.id_impediment)\n",
        "        self.impediment_type = 'PD'\n",
        "\n",
        "    def get_df(self, sheet):\n",
        "        if sheet == 'healthy':\n",
        "            return self.healthy_data\n",
        "        return self.impediment_data\n",
        "\n",
        "    def add_column(self,\n",
        "                   sheet_name: str,\n",
        "                   category: str,\n",
        "                   clusters: pd.Series) -> None:\n",
        "        \"\"\"\n",
        "        Adding a column with clusters\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            self.healthy_data[category] = clusters\n",
        "\n",
        "        else:\n",
        "            self.impediment_data[category] = clusters\n",
        "\n",
        "    def count_num_switches(self,\n",
        "                           sheet_name: str,\n",
        "                           category: str) -> None:\n",
        "        \"\"\"\n",
        "        Count number of switches for each cell\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            new_column_name = f'Switch_number_{category}'\n",
        "            self.healthy_data[new_column_name] = self.healthy_data[category].apply(lambda x: len(x) - 1)\n",
        "\n",
        "        else:\n",
        "            new_column_name = f'Switch_number_{category}'\n",
        "            self.impediment_data[new_column_name] = self.impediment_data[category].apply(lambda x: len(x) - 1)\n",
        "\n",
        "    def count_mean_cluster_size(self,\n",
        "                                sheet_name: str,\n",
        "                                category: str) -> None:\n",
        "        \"\"\"\n",
        "        Count mean cluster size for each row\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            new_column_name = f'Mean_cluster_size_{category}'\n",
        "            self.healthy_data[new_column_name] = self.healthy_data[category].apply(self.avg_cluster_size)\n",
        "\n",
        "        else:\n",
        "            new_column_name = f'Mean_cluster_size_{category}'\n",
        "            self.impediment_data[new_column_name] = self.impediment_data[category].apply(self.avg_cluster_size)\n",
        "\n",
        "    def count_mean_distances(self,\n",
        "                             sheet_name: str,\n",
        "                             category: str):\n",
        "        \"\"\"\n",
        "        Counting distances for all columns\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            new_column_name = f'Mean_distance_{category}'\n",
        "            self.healthy_data[new_column_name] = self.healthy_data[category].apply(self.avg_cluster_distance)\n",
        "\n",
        "        else:\n",
        "            new_column_name = f'Mean_distance_{category}'\n",
        "            self.impediment_data[new_column_name] = self.impediment_data[category].apply(self.avg_cluster_distance)\n",
        "\n",
        "    def count_mean_silhouette_score(self,\n",
        "                                    sheet_name: str,\n",
        "                                    category: str):\n",
        "        \"\"\"\n",
        "        Counting silhouette scores for all columns\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            new_column_name = f'Silhouette_score_{category}'\n",
        "            self.healthy_data[new_column_name] = self.healthy_data[category].apply(self.silhouette_score)\n",
        "\n",
        "        else:\n",
        "            new_column_name = f'Silhouette_score_{category}'\n",
        "            self.impediment_data[new_column_name] = self.impediment_data[category].apply(self.silhouette_score)\n",
        "\n",
        "    def count_cluster_t_scores(self,\n",
        "                               sheet_name: str,\n",
        "                               category: str):\n",
        "        \"\"\"\n",
        "        Counting cluster t-scores for all columns\n",
        "        \"\"\"\n",
        "        if sheet_name == 'healthy':\n",
        "            new_column_name = f'Mean_cluster_t_score_{category}'\n",
        "            self.healthy_data[new_column_name] = self.healthy_data[category].apply(\n",
        "                lambda x: self.avg_cluster_t_score(x, self.healthy_data[category])\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            new_column_name = f'Mean_cluster_t_score_{category}'\n",
        "            self.impediment_data[new_column_name] = self.impediment_data[category].apply(\n",
        "                lambda x: self.avg_cluster_t_score(x, self.impediment_data[category])\n",
        "            )"
      ],
      "metadata": {
        "id": "WL50LeRTBdvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O7Ikx7MA-aS"
      },
      "outputs": [],
      "source": [
        "class Clusterizer:\n",
        "\n",
        "    def __init__(self, model: gensim.models.fasttext.FastTextKeyedVectors) -> None:\n",
        "        self._model = model\n",
        "\n",
        "    def get_cosine_similarity(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Getting cosine similarity depending on model\n",
        "        \"\"\"\n",
        "        if isinstance(self._model, gensim.models.fasttext.FastTextKeyedVectors):\n",
        "            return self._model.similarity(w1, w2)\n",
        "\n",
        "        v1 = self._model.get_word_vector(w1)\n",
        "        v2 = self._model.get_word_vector(w2)\n",
        "\n",
        "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "    def cluster(self, word_sequence: list[str]) -> list[list[str]]:\n",
        "        \"\"\"\n",
        "        An implementation of words clustering algorithm\n",
        "        for verbal fluency test results,\n",
        "        from N. Lundin et al., 2022.\n",
        "        b -- current word\n",
        "        c -- next word\n",
        "        d -- next word after the next\n",
        "        \"\"\"\n",
        "        words_by_clusters = []\n",
        "        cluster = []\n",
        "\n",
        "        for idx, word in enumerate(word_sequence):\n",
        "            if word == 'PEOS':\n",
        "                break\n",
        "\n",
        "            if word == 'BOS':\n",
        "                b = word\n",
        "                c = word_sequence[idx + 1]\n",
        "                d = word_sequence[idx + 2]\n",
        "                b_c_sim = self.get_cosine_similarity(b, c)\n",
        "                c_d_sim = self.get_cosine_similarity(c, d)\n",
        "                continue\n",
        "\n",
        "            cluster.append(word)\n",
        "            a_b_sim = b_c_sim   # S(A,B) equals S(B,C) from previous iteration\n",
        "            b_c_sim = c_d_sim   # S(B,C) equals S(C,D) from previous iteration\n",
        "            c_d_sim = self.get_cosine_similarity(word_sequence[idx + 1], word_sequence[idx + 2])\n",
        "\n",
        "            if a_b_sim > b_c_sim and b_c_sim < c_d_sim:  # a condition of a switch\n",
        "                words_by_clusters.append(cluster)\n",
        "                cluster = []\n",
        "\n",
        "        if cluster:\n",
        "            words_by_clusters.append(cluster)\n",
        "        return words_by_clusters\n",
        "\n",
        "    @staticmethod\n",
        "    def _custom_similarity(embedding_1, embedding_2):  # с этим что-то надо сделать, оно работает для фасттехта?\n",
        "        return np.dot(gensim.matutils.unitvec(embedding_1),\n",
        "                      gensim.matutils.unitvec(embedding_2))\n",
        "\n",
        "    def davies_bouldin_index(self, cluster_sequence: list[list[str]]) -> float:\n",
        "        \"\"\"\n",
        "        The Davies Bouldin index implementation,\n",
        "        based on https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index\n",
        "        Si -- the average distance between each point of cluster i\n",
        "        and the centroid of that cluster – also known as cluster diameter;\n",
        "        Sj -- the average distance between each point of cluster j\n",
        "        and the centroid of that cluster – also known as cluster diameter;\n",
        "        Dij -- the distance between cluster centroids i and j;\n",
        "        Rij -- similarity between clusters i and j.\n",
        "        \"\"\"\n",
        "        centroids_dict = {}\n",
        "        for cluster in cluster_sequence:\n",
        "            centroid = sum(self.model[word] for word in cluster) / len(cluster)\n",
        "            centroids_dict[tuple(cluster)] = centroid\n",
        "\n",
        "        Si_values_dict = {}\n",
        "        for cluster in cluster_sequence:\n",
        "            cluster_centroid = centroids_dict[tuple(cluster)]\n",
        "            Si = sum(self._custom_similarity(self.model[word], cluster_centroid)\n",
        "                    for word in cluster) / len(cluster)\n",
        "            Si_values_dict[tuple(cluster)] = Si\n",
        "\n",
        "        Rij_max_values = []\n",
        "        for cluster_1 in cluster_sequence:\n",
        "            Rij_values = []\n",
        "            Si = Si_values_dict[tuple(cluster)]\n",
        "\n",
        "            for cluster_2 in cluster_sequence:\n",
        "                if cluster_2 == cluster_1:\n",
        "                    continue\n",
        "                Sj = Si_values_dict[tuple(cluster_2)]\n",
        "                Dij = self._custom_similarity(centroids_dict[tuple(cluster_1)], centroids_dict[tuple(cluster_2)])\n",
        "                Rij = (Si + Sj) / Dij\n",
        "                Rij_values.append(Rij)\n",
        "\n",
        "            if Rij_values:\n",
        "                Rij_max_values.append(max(Rij_values))\n",
        "\n",
        "        return sum(Rij_max_values) / len(Rij_max_values) if Rij_max_values else None\n",
        "\n",
        "    def silhouette_score(self, cluster_sequence: list[list[str]]) -> float:\n",
        "        \"\"\"\n",
        "        The Silhouette score implementation,\n",
        "        based on https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n",
        "        a -- the mean distance between a sample and all other points in the same class;\n",
        "        b -- the mean distance between a sample and all other points in the next nearest cluster.\n",
        "        \"\"\"\n",
        "        silhouette_coefs = []\n",
        "\n",
        "        for idx, cluster in enumerate(cluster_sequence):\n",
        "            for word_1 in cluster:\n",
        "\n",
        "                a = sum(self.get_cosine_similarity(word_1, word_2)\n",
        "                    for word_2 in cluster if word_1 != word_2) / len(cluster)\n",
        "\n",
        "                if idx != len(cluster_sequence) - 1:\n",
        "                    b = sum(self.get_cosine_similarity(word_1, word_2)\n",
        "                    for word_2 in cluster_sequence[idx + 1]) / len(cluster_sequence[idx + 1])\n",
        "                else:\n",
        "                    b = sum(self.get_cosine_similarity(word_1, word_2)\n",
        "                    for word_2 in cluster_sequence[idx - 1]) / len(cluster_sequence[idx - 1])\n",
        "\n",
        "                if a == 0 or b == 0:\n",
        "                    s = 0\n",
        "                else:\n",
        "                    s = (b - a) / max(a, b)\n",
        "                silhouette_coefs.append(s)\n",
        "\n",
        "        return sum(silhouette_coefs) / len(silhouette_coefs)\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_clustering(DB_values_page: list[float], silhouette_values: list[float]) -> None:\n",
        "        \"\"\"\n",
        "        The computation of all clustering metrics\n",
        "        to evaluate given clustering model.\n",
        "        \"\"\"\n",
        "        mean_DB_index_value = sum(DB_values_page) / len(DB_values_page)\n",
        "        mean_silhouette_score_value = sum(silhouette_values) / len(silhouette_values)\n",
        "        print('The performance of this clustering algorithm: ')\n",
        "        print(f'Mean value of Davies Bouldin index: {mean_DB_index_value}')\n",
        "        print(f'Mean value of Silhouette score: {mean_silhouette_score_value}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer:\n",
        "    def __init__(self, model: gensim.models.fasttext.FastTextKeyedVectors) -> None:\n",
        "        self.model = model\n",
        "        self._vectors_dictionary = {'BOS': self.model['BOS'].tolist(),\n",
        "                                    'EOS': self.model['EOS'].tolist(),\n",
        "                                    'PEOS': self.model['PEOS'].tolist()}\n",
        "\n",
        "    def update_dict(self, words: str) -> None:\n",
        "        \"\"\"\n",
        "        Updating the dictionary during each cell vectorising\n",
        "        \"\"\"\n",
        "        for one_word in words.split(', '):\n",
        "            if one_word not in self._vectors_dictionary:\n",
        "                self._vectors_dictionary[one_word] = self.model[one_word].tolist()\n",
        "\n",
        "    def update_json(self) -> None:\n",
        "        \"\"\"\n",
        "        Updating and saving the json file\n",
        "        \"\"\"\n",
        "        with open(\"/content/vectors.json\", \"w\") as fp:\n",
        "            json.dump(self._vectors_dictionary, fp, ensure_ascii=False)\n",
        "\n",
        "    def get_dictionary(self) -> dict:\n",
        "        \"\"\"\n",
        "        In case we need to get the dictionary\n",
        "        \"\"\"\n",
        "        return self._vectors_dictionary\n",
        "\n",
        "    @staticmethod\n",
        "    def get_sequence(words_string: str) -> list[str]:\n",
        "        \"\"\"\n",
        "        Getting a list of tokens + tags of beginning and ending\n",
        "        BOS -- Beginning of Sentence\n",
        "        PEOS -- pre-End of Sentence\n",
        "        EOS -- End of Sentence\n",
        "        \"\"\"\n",
        "        return ['BOS'] + words_string.split(', ') + ['PEOS', 'EOS']"
      ],
      "metadata": {
        "id": "mx10lcrUBd5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = r'' # добавьте свой путь до папки\n",
        "\n",
        "\n",
        "def main():\n",
        "    # defining classes\n",
        "    model_path = rf'{project_path}\\models\\geowac\\model.model'\n",
        "    geowac_model = gensim.models.KeyedVectors.load(model_path)\n",
        "    extractor = DataExtractionPDTexts(rf'{project_path}\\data\\control_pd_preprocessed.xlsx')\n",
        "    vectoriser = Vectorizer(geowac_model)\n",
        "    cluster_saver = ClustersDataPDTexts(extractor, geowac_model)\n",
        "    clusters_getter = Clusterizer(geowac_model)\n",
        "\n",
        "    # general principle: clustering one cell at a time\n",
        "    DB_values_page = []\n",
        "    silhouette_values_page = []\n",
        "\n",
        "    for page in ['healthy', 'pd']:\n",
        "        DB_values_lexemes_kind = []\n",
        "        silhouette_values_lexemes_kind = []\n",
        "\n",
        "        for category in extractor.category_types:\n",
        "            sequence_series = extractor.get_series(page, category)  # getting words lists from a column\n",
        "            clusters_list = []  # a list of lists of clusters for current column\n",
        "\n",
        "            DB_values_column = []\n",
        "            silhouette_values_column = []\n",
        "\n",
        "            for words_string in sequence_series:\n",
        "                if not isinstance(words_string, str):  # dealing with NaNs or other non-string values\n",
        "                    clusters_list.append([])\n",
        "                    continue\n",
        "\n",
        "                tokens_sequence = vectoriser.get_sequence(words_string)\n",
        "                # string of words coverted to list with special tags\n",
        "\n",
        "                cell_clusters = clusters_getter.cluster(tokens_sequence)\n",
        "                # converting list of words to list of clusters\n",
        "                clusters_list.append(cell_clusters)\n",
        "\n",
        "                DB_value = clusters_getter.davies_bouldin_index(cell_clusters)\n",
        "                # calculating Davies Bouldin index for each cell\n",
        "                if DB_value:\n",
        "                    DB_values_column.append(DB_value)\n",
        "\n",
        "                silhouette_value = clusters_getter.silhouette_score(cell_clusters)\n",
        "                # calculating Silhouette score for each cell\n",
        "                silhouette_values_column.append(silhouette_value)\n",
        "\n",
        "            cluster_saver.add_column(page, category,\n",
        "                                     pd.Series(clusters_list))\n",
        "            # adding clusters column in a table\n",
        "\n",
        "            # counting metrics\n",
        "            cluster_saver.count_num_switches(page, category)\n",
        "            cluster_saver.count_mean_cluster_size(page, category)\n",
        "            cluster_saver.count_mean_distances(page, category)\n",
        "            cluster_saver.count_mean_silhouette_score(page, category)\n",
        "            cluster_saver.count_cluster_t_scores(page, category)\n",
        "\n",
        "            DB_values_lexemes_kind.extend(DB_values_column)\n",
        "            silhouette_values_lexemes_kind.extend(silhouette_values_column)\n",
        "\n",
        "        DB_values_page.extend(DB_values_lexemes_kind)\n",
        "        silhouette_values_page.extend(silhouette_values_lexemes_kind)\n",
        "\n",
        "    clusters_getter.evaluate_clustering(DB_values_page, silhouette_values_page)\n",
        "    cluster_saver.save_excel(rf'{project_path}\\result\\pd_texts\\clusters_metrics_dataset.xlsx')\n",
        "    # vectors = vectoriser.get_dictionary()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "_XO5u2iZBQ7H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}